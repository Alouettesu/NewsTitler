{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfpQ4MqUGehCpTwlUh2F/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alouettesu/NewsTitler/blob/main/NewsTitler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Практическое задание: решение задачи обработки естественного языка\n",
        "------\n",
        "\n",
        "*Выполнил: Дианов Сергей Андреевич*\n",
        "\n",
        "Бывало ли у вас такое, что, читая новостную ленту, вы видите интересный заголовок, а открыв его, обнаруживаете пустышку? Или текст, совершенно не соответствующий названию? Текст, в котором заявленная в заголовке тема не раскрыта никак? \n",
        "\n",
        "У меня такое тоже бывает, и особенно этим грешат журналисты mail.ru. А что если сделать машину, которая сама придумывает заголовки для новостей? Пусть на вход программе подаётся текст новости, а на выходе генерируется заголовок. Это я и выбрал в качестве своего первого задания по обработке естественного языка. Заголовки генерируются с помощью модели для библиотеки HuggingFace [IlyaGusev/rut5_base_headline_gen_telegram](https://huggingface.co/IlyaGusev/rut5_base_headline_gen_telegram). Новости загружаются с RSS-ленты сайта mail.ru.\n",
        "\n",
        "Скрипт запрашивает содержимое RSS-файла и извлекает из него заголовки и ссылки на статьи. Затем загружает каждую веб-страницу по отдельности и извлекает из неё содержимое статьи путём нахождения html-тега `div class=\"article__text\"`. Но если формат веб-страницы на сайте изменится, то потребуется доработка скрипта. Тем не менее, вы можете задать самостоятельно URL RSS-ленты и критерии поиска текста в форме ниже.\n",
        "\n",
        "Результат работы выводится в стандартный вывод в форме таблицы. При запуске скрипта в какой-либо другой среде (не Google Colab) рекомендуется использовать терминал, который поддерживает регулировку ширины.\n",
        "\n",
        "*Примечание: Иногда Mail.ru возвращает ошибку `HTTP Error 429 \"Too many requests\"`. Страницы, которые уже загрузились до появления этой ошибки, обрабатываются нормально.*"
      ],
      "metadata": {
        "id": "orZbnLwrZTRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RSS_URL = \"https://news.mail.ru/rss/main/66/\" #@param {type:\"string\"}\n",
        "HTML_ELEMENT = \"div\" #@param {type:\"string\"}\n",
        "HTML_ATTRIB_NAME = \"class\" #@param [\"class\", \"id\"]\n",
        "HTML_ATTRIB_VALUE = \"article__text\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "RQyKWSjOfDvW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece sacremoses\n",
        "!pip install feedparser\n",
        "!pip install bs4\n",
        "!pip install html2text\n",
        "!pip install -U prettytable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzomEy9CKIk9",
        "outputId": "1e67b882-f3c2-4452-9f56-396d8001ca3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 41.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 42.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 36.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ad0536439ddec65151a8aa187363cbb94b6a28dc95779629a2eaccd31f810ea1\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, sacremoses\n",
            "Successfully installed huggingface-hub-0.9.1 sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.21.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=3dfa185224c6bc0f6b96481ec93f096790fda076042bed009b3a11236541c850\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.10 sgmllib3k-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting html2text\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: html2text\n",
            "Successfully installed html2text-2020.1.16\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (3.4.0)\n",
            "Collecting prettytable\n",
            "  Downloading prettytable-3.4.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable) (3.8.1)\n",
            "Installing collected packages: prettytable\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 3.4.0\n",
            "    Uninstalling prettytable-3.4.0:\n",
            "      Successfully uninstalled prettytable-3.4.0\n",
            "Successfully installed prettytable-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт модулей для работы с текстом\n",
        "import feedparser\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Импорт модулей для загрузки и извлечения текста из HTML\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "from urllib.error import HTTPError\n",
        "import html2text\n",
        "\n",
        "# Для вывода таблиц\n",
        "from prettytable import PrettyTable"
      ],
      "metadata": {
        "id": "tDTQ45VSL3Os"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция принимает на вход URL-адрес веб-страницы, загружает её содержимое,\n",
        "# ищет текст новости, удаляет из него HTML-тэги, возвращает текст новости.\n",
        "\n",
        "def getNewsText(news_url):\n",
        "  req = Request(news_url)\n",
        "  html_page = urlopen(req)\n",
        "\n",
        "  soup = BeautifulSoup(html_page, \"html.parser\")\n",
        "\n",
        "  txt = soup.find(HTML_ELEMENT, {HTML_ATTRIB_NAME : HTML_ATTRIB_VALUE})\n",
        "  \n",
        "  return txt.text\n"
      ],
      "metadata": {
        "id": "5-LgnuuEUv90"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Класс озаглавливателя новостей\n",
        "# В конструкторе класса загружается модель и токенизатор.\n",
        "# В методе Title на вход подаётся текст новости, на выходе выдаётся предложенный моделью заголовок.\n",
        "\n",
        "class Titler:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.model_name = \"IlyaGusev/rut5_base_headline_gen_telegram\"\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(self.model_name)\n",
        "\n",
        "  def Title(self, article_text):\n",
        "\n",
        "    input_ids = self.tokenizer(\n",
        "        article_text,\n",
        "        max_length=600,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    output_ids = self.model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens = 30\n",
        "    )[0]\n",
        "    headline = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "    return headline"
      ],
      "metadata": {
        "id": "5Uz2EpCyVKa8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Здесь основной код.\n",
        "\n",
        "# Загружается RSS-лента\n",
        "NewsFeed = feedparser.parse(RSS_URL)\n",
        "# Создание класса озаглавливателя\n",
        "titler = Titler()\n",
        "\n",
        "# Формируем заголовки таблицы для вывода результата работы скрипта\n",
        "columns = ['Исходный заголовок', 'Заголовок нейросети', 'Ссылка']\n",
        "tab = PrettyTable(columns)\n",
        "entryNum = 0\n",
        "\n",
        "for entry in NewsFeed.entries:\n",
        "  entryNum = entryNum + 1\n",
        "  print(\"Processing page\", entryNum, \"of\", len(NewsFeed.entries))\n",
        "  # Извлекаем ссылку и исходное наименование статьи из RSS-ленты\n",
        "  link = entry.link\n",
        "  MailRuTitle = entry.title\n",
        "  try:\n",
        "    # Загружаем текст новости\n",
        "    NewsText = getNewsText(link)\n",
        "  except HTTPError as e:\n",
        "    print(e.reason)\n",
        "    break;\n",
        "  # Запускаем модель для озаглавливания текста\n",
        "  GeneratedTitle = titler.Title(NewsText)\n",
        "  # Добавляем новую строку в таблицу\n",
        "  tab.add_row([MailRuTitle, GeneratedTitle, link])\n",
        "\n",
        "# Печатаем результат работы скрипта\n",
        "print(tab)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cje4HToCL1p7",
        "outputId": "61b8dd7e-0114-4cb0-c047-6679c3eb405f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 1 of 38\n",
            "Processing page 2 of 38\n",
            "Processing page 3 of 38\n",
            "Processing page 4 of 38\n",
            "Processing page 5 of 38\n",
            "Processing page 6 of 38\n",
            "Processing page 7 of 38\n",
            "Processing page 8 of 38\n",
            "Processing page 9 of 38\n",
            "Processing page 10 of 38\n",
            "Processing page 11 of 38\n",
            "Processing page 12 of 38\n",
            "Processing page 13 of 38\n",
            "Processing page 14 of 38\n",
            "Processing page 15 of 38\n",
            "Processing page 16 of 38\n",
            "Processing page 17 of 38\n",
            "Processing page 18 of 38\n",
            "Processing page 19 of 38\n",
            "Processing page 20 of 38\n",
            "Processing page 21 of 38\n",
            "Processing page 22 of 38\n",
            "Processing page 23 of 38\n",
            "Processing page 24 of 38\n",
            "Processing page 25 of 38\n",
            "Processing page 26 of 38\n",
            "Processing page 27 of 38\n",
            "Processing page 28 of 38\n",
            "Processing page 29 of 38\n",
            "Processing page 30 of 38\n",
            "Processing page 31 of 38\n"
          ]
        }
      ]
    }
  ]
}